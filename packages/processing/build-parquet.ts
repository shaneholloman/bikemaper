// Builds Parquet files from Citi Bike CSV trip data with embedded route geometries.
//
// Usage: bun run build-parquet.ts <year>
// Example: bun run build-parquet.ts 2025
//
// Prerequisites:
// - CSV files in data/<year>/**/*.csv
// - output/routes.db (from build-routes.ts)
//
// Output:
// - output/trips/<year>-01.parquet, <year>-02.parquet, etc.
import { DuckDBConnection } from "@duckdb/node-api";
import fs from "fs";
import { globSync } from "glob";
import path from "path";
import { dataDir, formatHumanReadableBytes, gitRoot } from "./utils";

type SchemaType = "legacy" | "modern";

async function detectSchemaType(data: {
  connection: DuckDBConnection;
  csvGlob: string;
}): Promise<SchemaType> {
  const { connection, csvGlob } = data;
  // Read column names from first CSV (sample_size=1 for speed)
  const result = await connection.runAndReadAll(`
    SELECT column_name
    FROM (DESCRIBE SELECT * FROM read_csv_auto('${csvGlob}', sample_size=1))
  `);
  const columns = result
    .getRowObjects()
    .map((r) => (r as { column_name: string }).column_name);

  // Legacy schema has 'tripduration', modern has 'ride_id'
  if (columns.includes("tripduration")) {
    return "legacy";
  }
  return "modern";
}

// Parse CLI argument
const targetYearArg = process.argv[2];
if (!targetYearArg || !/^\d{4}$/.test(targetYearArg)) {
  console.error("Usage: bun run build-parquet.ts <year>");
  console.error("Example: bun run build-parquet.ts 2025");
  process.exit(1);
}
const targetYear: string = targetYearArg;

const csvGlob = path.join(dataDir, `${targetYear}/**/*.csv`);

const outputDir = path.join(gitRoot, "packages/processing/output");
const routesDbPath = path.join(outputDir, "routes.db");

type ValidationResult = {
  total_rows: bigint;
  null_ride_id: bigint;
  null_start_station_id: bigint;
  null_end_station_id: bigint;
  null_start_station_name: bigint;
  null_end_station_name: bigint;
  null_started_at: bigint;
  null_ended_at: bigint;
  null_start_lat: bigint;
  null_start_lng: bigint;
  null_end_lat: bigint;
  null_end_lng: bigint;
  null_rideable_type: bigint;
  null_member_casual: bigint;
  unparseable_started_at: bigint;
  unparseable_ended_at: bigint;
  unparseable_start_lat: bigint;
  unparseable_start_lng: bigint;
  unparseable_end_lat: bigint;
  unparseable_end_lng: bigint;
  invalid_rideable_type: bigint;
  invalid_member_casual: bigint;
  end_before_start: bigint;
};

function printValidationWarnings(v: ValidationResult): void {
  const warnings: string[] = [];
  const total = Number(v.total_rows);

  const fmt = (count: bigint, msg: string) => {
    const pct = ((Number(count) / total) * 100).toFixed(2);
    return `${count} rows (${pct}%) with ${msg}`;
  };

  // NULL checks
  if (v.null_ride_id > 0) warnings.push(fmt(v.null_ride_id, "NULL ride_id"));
  if (v.null_start_station_id > 0) warnings.push(fmt(v.null_start_station_id, "NULL start_station_id"));
  if (v.null_end_station_id > 0) warnings.push(fmt(v.null_end_station_id, "NULL end_station_id"));
  if (v.null_start_station_name > 0) warnings.push(fmt(v.null_start_station_name, "NULL start_station_name"));
  if (v.null_end_station_name > 0) warnings.push(fmt(v.null_end_station_name, "NULL end_station_name"));
  if (v.null_started_at > 0) warnings.push(fmt(v.null_started_at, "NULL started_at"));
  if (v.null_ended_at > 0) warnings.push(fmt(v.null_ended_at, "NULL ended_at"));
  if (v.null_start_lat > 0) warnings.push(fmt(v.null_start_lat, "NULL start_lat"));
  if (v.null_start_lng > 0) warnings.push(fmt(v.null_start_lng, "NULL start_lng"));
  if (v.null_end_lat > 0) warnings.push(fmt(v.null_end_lat, "NULL end_lat"));
  if (v.null_end_lng > 0) warnings.push(fmt(v.null_end_lng, "NULL end_lng"));
  if (v.null_rideable_type > 0) warnings.push(fmt(v.null_rideable_type, "NULL rideable_type"));
  if (v.null_member_casual > 0) warnings.push(fmt(v.null_member_casual, "NULL member_casual"));

  // Type/parse checks
  if (v.unparseable_started_at > 0) warnings.push(fmt(v.unparseable_started_at, "unparseable started_at"));
  if (v.unparseable_ended_at > 0) warnings.push(fmt(v.unparseable_ended_at, "unparseable ended_at"));
  if (v.unparseable_start_lat > 0) warnings.push(fmt(v.unparseable_start_lat, "unparseable start_lat"));
  if (v.unparseable_start_lng > 0) warnings.push(fmt(v.unparseable_start_lng, "unparseable start_lng"));
  if (v.unparseable_end_lat > 0) warnings.push(fmt(v.unparseable_end_lat, "unparseable end_lat"));
  if (v.unparseable_end_lng > 0) warnings.push(fmt(v.unparseable_end_lng, "unparseable end_lng"));

  // Enum checks
  if (v.invalid_rideable_type > 0) warnings.push(fmt(v.invalid_rideable_type, "invalid rideable_type (must be 'classic_bike' or 'electric_bike')"));
  if (v.invalid_member_casual > 0) warnings.push(fmt(v.invalid_member_casual, "invalid member_casual (must be 'member' or 'casual')"));

  // Logic checks
  if (v.end_before_start > 0) warnings.push(fmt(v.end_before_start, "ended_at before started_at"));

  if (warnings.length > 0) {
    console.warn(`\nValidation warnings (rows will be dropped):\n  - ${warnings.join("\n  - ")}`);
  } else {
    console.log("No validation issues found.");
  }
}

async function main() {
  console.log("Starting parquet build...");
  console.log(`Data directory: ${dataDir}`);
  console.log(`Output directory: ${outputDir}`);

  // Ensure output directories exist
  fs.mkdirSync(path.join(outputDir, "trips"), { recursive: true });

  const connection = await DuckDBConnection.create();

  // Configure DuckDB for large workloads - spill to disk instead of using 40GB RAM
  const tempDir = path.join(outputDir, "duckdb_tmp");
  fs.mkdirSync(tempDir, { recursive: true });
  await connection.run(`SET temp_directory = '${tempDir}'`);
  await connection.run(`SET memory_limit = '32GB'`);
  await connection.run(`SET preserve_insertion_order = false`);

  // 1. Load ALL data without filtering (validation will catch issues)
  console.log(`\nReading CSVs matching: ${csvGlob}`);

  // Expand glob so we can report inputs deterministically
  const matchedCsvs = globSync(csvGlob, { nodir: true });
  if (matchedCsvs.length === 0) {
    throw new Error(`No CSV files matched: ${csvGlob}`);
  }

  let totalBytes = 0;
  for (const filePath of matchedCsvs) {
    totalBytes += fs.statSync(filePath).size;
  }

  console.log(`Matched CSVs: ${matchedCsvs.length}`);
  console.log(matchedCsvs.map((p) => `- ${p}`).join("\n"));
  console.log(`Total input size: ${formatHumanReadableBytes(totalBytes)}`);

  const startTime = Date.now();

  // Detect schema type from CSV columns
  const schemaType = await detectSchemaType({ connection, csvGlob });
  console.log(`Detected schema type: ${schemaType}`);

  if (schemaType === "legacy") {
    // Legacy schema (2013, 2018): normalize to modern format
    // We only need station names (for route lookup via stations.json) - legacy IDs are useless
    // union_by_name=true handles type mismatches between files
    await connection.run(`
      CREATE TEMP TABLE raw AS
      SELECT
        md5(bikeid::VARCHAR || starttime::VARCHAR) as ride_id,
        'classic_bike' as rideable_type,
        starttime as started_at,
        stoptime as ended_at,
        "start station name" as start_station_name,
        "end station name" as end_station_name,
        TRY_CAST("start station latitude" AS DOUBLE) as start_lat,
        TRY_CAST("start station longitude" AS DOUBLE) as start_lng,
        TRY_CAST("end station latitude" AS DOUBLE) as end_lat,
        TRY_CAST("end station longitude" AS DOUBLE) as end_lng,
        CASE WHEN usertype = 'Subscriber' THEN 'member' ELSE 'casual' END as member_casual
      FROM read_csv_auto('${csvGlob}', union_by_name=true)
    `);
  } else {
    // Modern schema (2020+): use columns directly
    await connection.run(`
      CREATE TEMP TABLE raw AS
      SELECT
        ride_id,
        rideable_type,
        started_at,
        ended_at,
        start_station_name,
        start_station_id,
        end_station_name,
        end_station_id,
        start_lat,
        start_lng,
        end_lat,
        end_lng,
        member_casual
      FROM read_csv_auto('${csvGlob}')
    `);
  }

  const loadTime = Date.now() - startTime;
  console.log(`Loaded CSVs into temp table in ${(loadTime / 1000).toFixed(1)}s`);

  // 2. Validate data
  console.log("\nValidating data...");

  // Station ID checks only apply to modern schema (legacy doesn't have them)
  const stationIdValidation =
    schemaType === "legacy"
      ? `0::BIGINT as null_start_station_id, 0::BIGINT as null_end_station_id,`
      : `COUNT(*) FILTER (WHERE start_station_id IS NULL) as null_start_station_id,
         COUNT(*) FILTER (WHERE end_station_id IS NULL) as null_end_station_id,`;

  const validationReader = await connection.runAndReadAll(`
    SELECT
      -- NULL checks
      COUNT(*) FILTER (WHERE ride_id IS NULL) as null_ride_id,
      ${stationIdValidation}
      COUNT(*) FILTER (WHERE start_station_name IS NULL) as null_start_station_name,
      COUNT(*) FILTER (WHERE end_station_name IS NULL) as null_end_station_name,
      COUNT(*) FILTER (WHERE started_at IS NULL) as null_started_at,
      COUNT(*) FILTER (WHERE ended_at IS NULL) as null_ended_at,
      COUNT(*) FILTER (WHERE start_lat IS NULL) as null_start_lat,
      COUNT(*) FILTER (WHERE start_lng IS NULL) as null_start_lng,
      COUNT(*) FILTER (WHERE end_lat IS NULL) as null_end_lat,
      COUNT(*) FILTER (WHERE end_lng IS NULL) as null_end_lng,
      COUNT(*) FILTER (WHERE rideable_type IS NULL) as null_rideable_type,
      COUNT(*) FILTER (WHERE member_casual IS NULL) as null_member_casual,

      -- Type checks (TRY_CAST returns NULL if unparseable)
      COUNT(*) FILTER (WHERE TRY_CAST(started_at AS TIMESTAMP) IS NULL AND started_at IS NOT NULL) as unparseable_started_at,
      COUNT(*) FILTER (WHERE TRY_CAST(ended_at AS TIMESTAMP) IS NULL AND ended_at IS NOT NULL) as unparseable_ended_at,
      COUNT(*) FILTER (WHERE TRY_CAST(start_lat AS DOUBLE) IS NULL AND start_lat IS NOT NULL) as unparseable_start_lat,
      COUNT(*) FILTER (WHERE TRY_CAST(start_lng AS DOUBLE) IS NULL AND start_lng IS NOT NULL) as unparseable_start_lng,
      COUNT(*) FILTER (WHERE TRY_CAST(end_lat AS DOUBLE) IS NULL AND end_lat IS NOT NULL) as unparseable_end_lat,
      COUNT(*) FILTER (WHERE TRY_CAST(end_lng AS DOUBLE) IS NULL AND end_lng IS NOT NULL) as unparseable_end_lng,

      -- Enum checks
      COUNT(*) FILTER (WHERE rideable_type NOT IN ('classic_bike', 'electric_bike')) as invalid_rideable_type,
      COUNT(*) FILTER (WHERE member_casual NOT IN ('member', 'casual')) as invalid_member_casual,

      -- Logic checks
      COUNT(*) FILTER (WHERE ended_at < started_at) as end_before_start,

      -- Total
      COUNT(*) as total_rows
    FROM raw
  `);

  const validation = validationReader.getRowObjects()[0] as ValidationResult;
  console.log(`Total rows: ${validation.total_rows}`);

  // Check for duplicates
  const duplicateReader = await connection.runAndReadAll(`
    SELECT COUNT(*) as duplicate_count
    FROM (
      SELECT ride_id
      FROM raw
      GROUP BY ride_id
      HAVING COUNT(*) > 1
    )
  `);

  const duplicateCount = Number(
    (duplicateReader.getRowObjects()[0] as { duplicate_count: bigint }).duplicate_count
  );
  if (duplicateCount > 0) {
    const pct = ((duplicateCount / Number(validation.total_rows)) * 100).toFixed(2);
    console.warn(`\nWarning: ${duplicateCount} duplicate ride_ids (${pct}%) will be deduplicated`);
  }

  // Print validation warnings
  printValidationWarnings(validation);

  // 3. Load routes from SQLite into DuckDB
  console.log("\nLoading routes from SQLite...");
  if (!fs.existsSync(routesDbPath)) {
    throw new Error(`routes.db not found at ${routesDbPath}. Run build-routes.ts first.`);
  }
  await connection.run(`
    INSTALL sqlite;
    LOAD sqlite;
  `);
  await connection.run(`
    CREATE TABLE routes AS
    SELECT * FROM sqlite_scan('${routesDbPath}', 'routes')
  `);
  const routeCountReader = await connection.runAndReadAll(`SELECT COUNT(*) as count FROM routes`);
  const routeCount = Number((routeCountReader.getRowObjects()[0] as { count: bigint }).count);
  console.log(`  ${routeCount} routes loaded`);

  // Load station data from stations.json for route matching
  // Routes are keyed by station NAME (not ID) because station IDs change between years
  const stationsJsonPath = path.join(gitRoot, "apps/client/public/stations.json");
  if (!fs.existsSync(stationsJsonPath)) {
    console.warn(`stations.json not found - trips will have no routes`);
  } else {
    // Create name normalization lookup: maps any name (canonical OR alias) -> canonical name
    // This handles station names that changed over time (e.g., "8 Ave & W 31 St" -> "W 31 St & 8 Ave")
    console.log("\nLoading station name lookup from stations.json...");
    await connection.run(`
      CREATE TABLE station_name_lookup AS
      -- Canonical names map to themselves
      SELECT name as any_name, name as canonical_name
      FROM read_json_auto('${stationsJsonPath}')
      UNION ALL
      -- Aliases map to their canonical name
      SELECT UNNEST(aliases) as any_name, name as canonical_name
      FROM read_json_auto('${stationsJsonPath}')
    `);
    const lookupCountReader = await connection.runAndReadAll(`SELECT COUNT(*) as count FROM station_name_lookup`);
    const lookupCount = Number((lookupCountReader.getRowObjects()[0] as { count: bigint }).count);
    console.log(`  ${lookupCount} name mappings loaded (canonical + aliases)`);

    if (schemaType === "legacy") {
      // Legacy data also needs coordinate-based matching since names may not match exactly
      console.log("Loading station coordinates for legacy coordinate matching...");
      await connection.run(`
        CREATE TABLE station_coords AS
        SELECT
          name as station_name,
          latitude,
          longitude
        FROM read_json_auto('${stationsJsonPath}')
      `);
      const coordCountReader = await connection.runAndReadAll(`SELECT COUNT(*) as count FROM station_coords`);
      const coordCount = Number((coordCountReader.getRowObjects()[0] as { count: bigint }).count);
      console.log(`  ${coordCount} stations loaded for coordinate matching`);
    }
  }

  // 4. Create trips table with routes via SQL JOIN (no JS iteration!)
  console.log("\nJoining trips with routes...");

  // NYC bounding box
  const NYC_BOUNDS = {
    minLat: 40.3,
    maxLat: 41.2,
    minLng: -74.5,
    maxLng: -73.5,
  };

  // Legacy data doesn't have station IDs (we get them from name lookup later)
  const stationIdFilter =
    schemaType === "legacy"
      ? ""
      : `AND start_station_id IS NOT NULL AND end_station_id IS NOT NULL`;

  const validRowFilter = `
    ride_id IS NOT NULL
    ${stationIdFilter}
    AND start_station_name IS NOT NULL
    AND end_station_name IS NOT NULL
    AND started_at IS NOT NULL
    AND ended_at IS NOT NULL
    AND start_lat IS NOT NULL
    AND start_lng IS NOT NULL
    AND end_lat IS NOT NULL
    AND end_lng IS NOT NULL
    AND rideable_type IN ('classic_bike', 'electric_bike')
    AND member_casual IN ('member', 'casual')
    AND ended_at >= started_at
    AND start_lat BETWEEN ${NYC_BOUNDS.minLat} AND ${NYC_BOUNDS.maxLat}
    AND start_lng BETWEEN ${NYC_BOUNDS.minLng} AND ${NYC_BOUNDS.maxLng}
    AND end_lat BETWEEN ${NYC_BOUNDS.minLat} AND ${NYC_BOUNDS.maxLat}
    AND end_lng BETWEEN ${NYC_BOUNDS.minLng} AND ${NYC_BOUNDS.maxLng}
  `;

  // Step 1: Filter valid rows
  console.log("  Step 1: Filtering valid rows...");
  let stepStart = Date.now();
  await connection.run(`
    CREATE TABLE filtered AS
    SELECT * FROM raw WHERE ${validRowFilter}
  `);
  console.log(`    Done in ${((Date.now() - stepStart) / 1000).toFixed(1)}s`);

  // Step 2: Deduplicate by ride_id
  console.log("  Step 2: Deduplicating by ride_id...");
  stepStart = Date.now();
  await connection.run(`
    CREATE TABLE deduped AS
    SELECT DISTINCT ON (ride_id) * FROM filtered
  `);
  console.log(`    Done in ${((Date.now() - stepStart) / 1000).toFixed(1)}s`);

  // Step 3: JOIN with routes
  console.log("  Step 3: Joining with routes...");
  stepStart = Date.now();

  if (schemaType === "legacy") {
    // Legacy data: match station coordinates to nearest current station (~200m threshold)
    // Routes are keyed by station NAME, so coordinate snap returns station_name
    // 0.002 lat = ~220m, 0.003 lng = ~230m at NYC latitude
    await connection.run(`
      CREATE TABLE joined AS
      SELECT
        t.ride_id as id,
        sc_start.station_name as startStationName,
        sc_end.station_name as endStationName,
        -- Convert naive local time (America/New_York) to UTC
        (t.started_at AT TIME ZONE 'America/New_York') AT TIME ZONE 'UTC' as startedAt,
        (t.ended_at AT TIME ZONE 'America/New_York') AT TIME ZONE 'UTC' as endedAt,
        t.rideable_type as bikeType,
        t.member_casual as memberCasual,
        t.start_lat as startLat,
        t.start_lng as startLng,
        t.end_lat as endLat,
        t.end_lng as endLng,
        r.geometry as routeGeometry,
        r.distance as routeDistance
      FROM deduped t
      -- Find nearest station by coordinates for start
      LEFT JOIN LATERAL (
        SELECT station_name
        FROM station_coords
        WHERE ABS(t.start_lat - latitude) < 0.002
          AND ABS(t.start_lng - longitude) < 0.003
        ORDER BY (t.start_lat - latitude)*(t.start_lat - latitude) +
                 (t.start_lng - longitude)*(t.start_lng - longitude)
        LIMIT 1
      ) sc_start ON true
      -- Find nearest station by coordinates for end
      LEFT JOIN LATERAL (
        SELECT station_name
        FROM station_coords
        WHERE ABS(t.end_lat - latitude) < 0.002
          AND ABS(t.end_lng - longitude) < 0.003
        ORDER BY (t.end_lat - latitude)*(t.end_lat - latitude) +
                 (t.end_lng - longitude)*(t.end_lng - longitude)
        LIMIT 1
      ) sc_end ON true
      LEFT JOIN routes r
        ON sc_start.station_name = r.start_station_name
        AND sc_end.station_name = r.end_station_name
    `);
  } else {
    // Modern data: normalize CSV station names to canonical names via lookup table
    // This handles names that changed over time (e.g., "8 Ave & W 31 St" -> "W 31 St & 8 Ave")
    // Routes are keyed by canonical name, so we must normalize before joining
    await connection.run(`
      CREATE TABLE joined AS
      SELECT
        t.ride_id as id,
        -- Use canonical name for station fields (for client lookup in stations.json)
        COALESCE(snl_start.canonical_name, t.start_station_name) as startStationName,
        COALESCE(snl_end.canonical_name, t.end_station_name) as endStationName,
        -- Convert naive local time (America/New_York) to UTC
        (t.started_at AT TIME ZONE 'America/New_York') AT TIME ZONE 'UTC' as startedAt,
        (t.ended_at AT TIME ZONE 'America/New_York') AT TIME ZONE 'UTC' as endedAt,
        t.rideable_type as bikeType,
        t.member_casual as memberCasual,
        t.start_lat as startLat,
        t.start_lng as startLng,
        t.end_lat as endLat,
        t.end_lng as endLng,
        r.geometry as routeGeometry,
        r.distance as routeDistance
      FROM deduped t
      -- Normalize start station name: CSV name -> canonical name
      LEFT JOIN station_name_lookup snl_start
        ON t.start_station_name = snl_start.any_name
      -- Normalize end station name: CSV name -> canonical name
      LEFT JOIN station_name_lookup snl_end
        ON t.end_station_name = snl_end.any_name
      -- Join routes on canonical names
      LEFT JOIN routes r
        ON COALESCE(snl_start.canonical_name, t.start_station_name) = r.start_station_name
        AND COALESCE(snl_end.canonical_name, t.end_station_name) = r.end_station_name
    `);
  }
  console.log(`    Done in ${((Date.now() - stepStart) / 1000).toFixed(1)}s`);

  // Step 4: Sort by date (enables row group pruning for date-range queries)
  console.log("  Step 4: Sorting by startedAt...");
  stepStart = Date.now();
  await connection.run(`
    CREATE TABLE trips AS
    SELECT * FROM joined ORDER BY startedAt
  `);
  console.log(`    Done in ${((Date.now() - stepStart) / 1000).toFixed(1)}s`);

  // Get stats
  const statsReader = await connection.runAndReadAll(`
    SELECT
      COUNT(*) as total,
      COUNT(routeGeometry) as with_route
    FROM trips
  `);
  const stats = statsReader.getRowObjects()[0] as { total: bigint; with_route: bigint };
  const tripCount = Number(stats.total);
  const withRoute = Number(stats.with_route);
  const withoutRoute = tripCount - withRoute;
  console.log(`  ${tripCount} trips created (${withRoute} with routes, ${withoutRoute} without)`);

  // 5. Export to monthly parquet files
  console.log("\nExporting to monthly Parquet files...");

  // Get distinct months in the data
  const monthsResult = await connection.runAndReadAll(`
    SELECT DISTINCT strftime(startedAt, '%Y-%m') as month
    FROM trips
    ORDER BY month
  `);

  const months = monthsResult.getRowObjects() as Array<{ month: string }>;
  console.log(`Found ${months.length} months of data: ${months.map((m) => m.month).join(", ")}`);

  let totalParquetBytes = 0;

  for (const { month } of months) {
    // Skip months not in target year (e.g., Dec 2024 trips in 2025 CSVs)
    if (!month.startsWith(targetYear)) {
      console.log(`  Skipping ${month} (not in target year ${targetYear})`);
      continue;
    }

    const parts = month.split("-");
    const year = parts[0];
    const monthNum = parts[1];
    if (!year || !monthNum) {
      throw new Error(`Invalid month format: ${month}`);
    }
    const nextMonth =
      monthNum === "12"
        ? `${parseInt(year) + 1}-01`
        : `${year}-${String(parseInt(monthNum) + 1).padStart(2, "0")}`;

    const monthPath = path.join(outputDir, `trips/${month}.parquet`);
    await connection.run(`
      COPY (
        SELECT * FROM trips
        WHERE startedAt >= '${month}-01'::TIMESTAMP
          AND startedAt < '${nextMonth}-01'::TIMESTAMP
      ) TO '${monthPath}' (FORMAT PARQUET, COMPRESSION ZSTD)
    `);

    const monthStats = fs.statSync(monthPath);
    totalParquetBytes += monthStats.size;
    console.log(`  ${month}.parquet: ${(monthStats.size / 1024 / 1024).toFixed(1)} MB`);
  }

  const droppedCount = Number(validation.total_rows) - tripCount;
  const droppedPct = ((droppedCount / Number(validation.total_rows)) * 100).toFixed(2);
  console.warn(`Total data loss: ${droppedCount} rows (${droppedPct}%) dropped`);

  console.log(`\nTotal Parquet output: ${(totalParquetBytes / 1024 / 1024).toFixed(1)} MB across ${months.length} files`);
  console.log(`  ${withRoute} trips with routes (${((withRoute / tripCount) * 100).toFixed(1)}%)`);
  console.log(`  ${withoutRoute} trips without routes`);

  connection.closeSync();

  // Clean up temp directory
  fs.rmSync(tempDir, { recursive: true, force: true });

  const totalTime = Date.now() - startTime;
  console.log(`\nDone in ${(totalTime / 1000).toFixed(1)}s`);
}

main().catch((err) => {
  console.error(err);
  process.exit(1);
});
